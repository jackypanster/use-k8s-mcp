"""
K8s MCP Agent LLMé…ç½®æ¨¡å—
ä¸“ç”¨äº Gemini 2.5 Flash æ¨¡å‹çš„ç¯å¢ƒé…ç½®ç®¡ç†
éµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®ºï¼Œæ‰€æœ‰é…ç½®é€šè¿‡ç¯å¢ƒå˜é‡ç®¡ç†
"""

import os
from typing import Dict, Any
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class GeminiMaxConfig:
    """
    Gemini 2.5 Flash ç¯å¢ƒé…ç½®
    ä»ç¯å¢ƒå˜é‡è¯»å–æ‰€æœ‰é…ç½®ï¼Œéµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®º
    """

    def __init__(self):
        """åˆå§‹åŒ–Geminié…ç½®ç®¡ç†å™¨ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–æ‰€æœ‰é…ç½®"""
        # éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
        self._validate_required_env_vars()

        # ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
        self._load_config_from_env()

    def _validate_required_env_vars(self):
        """éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡ï¼Œéµå¾ªfail-faståŸåˆ™"""
        required_vars = [
            "OPENROUTER_API_KEY",
            "OPENROUTER_BASE_URL",
            "LLM_MODEL_NAME"
        ]

        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)

        if missing_vars:
            raise ValueError(
                f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}. "
                f"è¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®ã€‚"
            )

    def _load_config_from_env(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½æ‰€æœ‰é…ç½®"""
        # LLM æä¾›å•†é…ç½®
        self.PROVIDER_NAME = os.getenv("LLM_PROVIDER_NAME", "OpenRouter")
        self.MODEL_NAME = os.getenv("LLM_MODEL_NAME")
        self.BASE_URL = os.getenv("OPENROUTER_BASE_URL")

        # æ¨¡å‹èƒ½åŠ›é…ç½®
        self.MAX_INPUT_CONTEXT = int(os.getenv("LLM_MAX_INPUT_CONTEXT", "1048576"))
        self.MAX_OUTPUT_TOKENS = int(os.getenv("LLM_MAX_OUTPUT_TOKENS", "32768"))
        self.MAX_TIMEOUT = int(os.getenv("LLM_REQUEST_TIMEOUT", "600"))

        # æ¨¡å‹è¡Œä¸ºé…ç½®
        self.TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.0"))
        self.TOP_P = float(os.getenv("LLM_TOP_P", "0.05"))
        self.MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", "5"))
        self.SEED = int(os.getenv("LLM_SEED", "42"))

        # å®‰å…¨é…ç½®
        safety_sequences = os.getenv("LLM_SAFETY_STOP_SEQUENCES",
                                   "```bash,```sh,```shell,rm -rf,kubectl delete,docker rmi,sudo rm")
        self.SAFETY_STOP_SEQUENCES = [seq.strip() for seq in safety_sequences.split(",")]

    def create_llm(self, **kwargs) -> ChatOpenAI:
        """
        åˆ›å»ºGemini 2.5 Flash LLMå®ä¾‹
        æ‰€æœ‰é…ç½®ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒè¿è¡Œæ—¶è¦†ç›–

        Args:
            **kwargs: å¯é€‰çš„è¦†ç›–å‚æ•°

        Returns:
            é…ç½®çš„Gemini 2.5 Flash ChatOpenAIå®ä¾‹
        """
        # åˆ›å»ºåŸå§‹LLM
        llm = ChatOpenAI(
            model=kwargs.get("model", self.MODEL_NAME),
            api_key=os.getenv("OPENROUTER_API_KEY"),
            base_url=kwargs.get("base_url", self.BASE_URL),

            # æ¨¡å‹èƒ½åŠ›é…ç½® (ä»ç¯å¢ƒå˜é‡)
            max_tokens=kwargs.get("max_tokens", self.MAX_OUTPUT_TOKENS),
            temperature=kwargs.get("temperature", self.TEMPERATURE),
            top_p=kwargs.get("top_p", self.TOP_P),

            # ç¨³å®šæ€§é…ç½®
            frequency_penalty=kwargs.get("frequency_penalty", 0.0),
            presence_penalty=kwargs.get("presence_penalty", 0.0),
            streaming=kwargs.get("streaming", False),
            seed=kwargs.get("seed", self.SEED),  # æ˜ç¡®æŒ‡å®šseedå‚æ•°ï¼Œä¿®å¤UserWarning

            # å¯é æ€§é…ç½® (ä»ç¯å¢ƒå˜é‡)
            max_retries=kwargs.get("max_retries", self.MAX_RETRIES),
            request_timeout=kwargs.get("request_timeout", self.MAX_TIMEOUT),

            # å®‰å…¨é…ç½® (ä»ç¯å¢ƒå˜é‡)
            stop=kwargs.get("stop", self.SAFETY_STOP_SEQUENCES),
        )
        
        # åŒ…è£…ä¸ºè¿½è¸ªç‰ˆæœ¬
        return llm
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ (ä»ç¯å¢ƒå˜é‡é…ç½®)"""
        api_key = os.getenv("OPENROUTER_API_KEY", "")
        masked_key = api_key[:10] + "..." if api_key else "æœªè®¾ç½®"

        return {
            "provider": self.PROVIDER_NAME,
            "model": self.MODEL_NAME,
            "api_key": masked_key,
            "base_url": self.BASE_URL,
            "input_context": f"{self.MAX_INPUT_CONTEXT:,} tokens",
            "output_tokens": f"{self.MAX_OUTPUT_TOKENS:,} tokens",
            "timeout": f"{self.MAX_TIMEOUT}s",
            "temperature": self.TEMPERATURE,
            "top_p": self.TOP_P,
            "max_retries": self.MAX_RETRIES,
            "features": ["å·¥å…·è°ƒç”¨", "å¤§ä¸Šä¸‹æ–‡", "æ¨ç†æ¨¡å¼", "K8sè¿ç»´"],
            "configuration": "ç¯å¢ƒå˜é‡é…ç½® - éµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®º"
        }


# å…¨å±€é…ç½®å®ä¾‹ - ä»ç¯å¢ƒå˜é‡åˆå§‹åŒ–
gemini_config = GeminiMaxConfig()


def create_llm(**kwargs) -> ChatOpenAI:
    """
    ğŸ¯ ä¸»è¦å…¥å£ç‚¹ï¼šåˆ›å»ºGemini 2.5 Flash LLMå®ä¾‹

    ä»ç¯å¢ƒå˜é‡è¯»å–æ‰€æœ‰é…ç½®ï¼Œéµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®ºã€‚
    æ”¯æŒä¸åŒéƒ¨ç½²ç¯å¢ƒçš„çµæ´»é…ç½®ã€‚

    é…ç½®æ¥æºï¼š
    - æ¨¡å‹åç§°ï¼šLLM_MODEL_NAME
    - è¾“å…¥ä¸Šä¸‹æ–‡ï¼šLLM_MAX_INPUT_CONTEXT
    - è¾“å‡ºèƒ½åŠ›ï¼šLLM_MAX_OUTPUT_TOKENS
    - è¶…æ—¶æ—¶é—´ï¼šLLM_REQUEST_TIMEOUT
    - æ¸©åº¦è®¾ç½®ï¼šLLM_TEMPERATURE

    Args:
        **kwargs: å¯é€‰çš„è¦†ç›–å‚æ•°

    Returns:
        é…ç½®çš„Gemini 2.5 Flash ChatOpenAIå®ä¾‹

    Raises:
        ValueError: å½“å¿…éœ€çš„ç¯å¢ƒå˜é‡æœªè®¾ç½®æ—¶

    Examples:
        >>> llm = create_llm()  # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        >>> llm = create_llm(max_tokens=16384)  # è¦†ç›–è¾“å‡ºé™åˆ¶
    """
    return gemini_config.create_llm(**kwargs)


def print_model_status():
    """æ‰“å°å½“å‰æ¨¡å‹çŠ¶æ€"""
    info = gemini_config.get_model_info()
    print(f"ğŸ¤– å½“å‰LLMæ¨¡å‹: {info['model']}")
    print(f"ğŸ”— æœåŠ¡åœ°å€: {info['base_url']}")
    print(f"ğŸ”‘ APIå¯†é’¥: {info['api_key']}")
    print(f"ğŸ“ è¾“å…¥ä¸Šä¸‹æ–‡: {info['input_context']}")
    print(f"ğŸ“¤ è¾“å‡ºèƒ½åŠ›: {info['output_tokens']}")
    print(f"â±ï¸  è¶…æ—¶è®¾ç½®: {info['timeout']}")
    print(f"ğŸ› ï¸  åŠŸèƒ½ç‰¹æ€§: {', '.join(info['features'])}")
    print(f"âš™ï¸  é…ç½®æ¨¡å¼: {info['configuration']}")
