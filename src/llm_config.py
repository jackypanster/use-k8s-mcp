"""
K8s MCP Agent LLMé…ç½®æ¨¡å—
ä¸“ç”¨äº Gemini 2.5 Flash æ¨¡å‹çš„ç¯å¢ƒé…ç½®ç®¡ç†
éµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®ºï¼Œæ‰€æœ‰é…ç½®é€šè¿‡ç¯å¢ƒå˜é‡ç®¡ç†
"""

import os
from typing import Dict, Any
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class GeminiMaxConfig:
    """
    Gemini 2.5 Flash ç¯å¢ƒé…ç½®
    ä»ç¯å¢ƒå˜é‡è¯»å–æ‰€æœ‰é…ç½®ï¼Œéµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®º
    """

    def __init__(self):
        """åˆå§‹åŒ–Geminié…ç½®ç®¡ç†å™¨ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–æ‰€æœ‰é…ç½®"""
        # éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
        self._validate_required_env_vars()

        # ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
        self._load_config_from_env()

    def _validate_required_env_vars(self):
        """éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡ï¼Œéµå¾ªfail-faståŸåˆ™"""
        required_vars = [
            "OPENROUTER_API_KEY",
            "OPENROUTER_BASE_URL",
            "LLM_MODEL_NAME"
        ]

        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)

        if missing_vars:
            raise ValueError(
                f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}. "
                f"è¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®ã€‚"
            )

    def _load_config_from_env(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½æ‰€æœ‰é…ç½®"""
        # LLM æä¾›å•†é…ç½®
        self.PROVIDER_NAME = os.getenv("LLM_PROVIDER_NAME", "OpenRouter")
        self.MODEL_NAME = os.getenv("LLM_MODEL_NAME")
        self.BASE_URL = os.getenv("OPENROUTER_BASE_URL")

        # æ¨¡å‹èƒ½åŠ›é…ç½®
        self.MAX_INPUT_CONTEXT = int(os.getenv("LLM_MAX_INPUT_CONTEXT", "1048576"))
        self.MAX_OUTPUT_TOKENS = int(os.getenv("LLM_MAX_OUTPUT_TOKENS", "32768"))
        self.MAX_TIMEOUT = int(os.getenv("LLM_REQUEST_TIMEOUT", "600"))

        # æ¨¡å‹è¡Œä¸ºé…ç½®
        self.TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.0"))
        self.TOP_P = float(os.getenv("LLM_TOP_P", "0.05"))
        self.MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", "5"))
        self.SEED = int(os.getenv("LLM_SEED", "42"))

        # å®‰å…¨é…ç½®
        safety_sequences = os.getenv("LLM_SAFETY_STOP_SEQUENCES",
                                   "```bash,```sh,```shell,rm -rf,kubectl delete,docker rmi,sudo rm")
        self.SAFETY_STOP_SEQUENCES = [seq.strip() for seq in safety_sequences.split(",")]

    def create_llm(self, **kwargs) -> ChatOpenAI:
        """
        åˆ›å»ºGemini 2.5 Flash LLMå®ä¾‹
        æ‰€æœ‰é…ç½®ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒè¿è¡Œæ—¶è¦†ç›–

        Args:
            **kwargs: å¯é€‰çš„è¦†ç›–å‚æ•°

        Returns:
            é…ç½®çš„Gemini 2.5 Flash ChatOpenAIå®ä¾‹
        """
        # åˆ›å»ºåŸå§‹LLM
        original_llm = ChatOpenAI(
            model=kwargs.get("model", self.MODEL_NAME),
            api_key=os.getenv("OPENROUTER_API_KEY"),
            base_url=kwargs.get("base_url", self.BASE_URL),

            # æ¨¡å‹èƒ½åŠ›é…ç½® (ä»ç¯å¢ƒå˜é‡)
            max_tokens=kwargs.get("max_tokens", self.MAX_OUTPUT_TOKENS),
            temperature=kwargs.get("temperature", self.TEMPERATURE),
            top_p=kwargs.get("top_p", self.TOP_P),

            # ç¨³å®šæ€§é…ç½®
            frequency_penalty=kwargs.get("frequency_penalty", 0.0),
            presence_penalty=kwargs.get("presence_penalty", 0.0),
            streaming=kwargs.get("streaming", False),
            seed=kwargs.get("seed", self.SEED),  # æ˜ç¡®æŒ‡å®šseedå‚æ•°ï¼Œä¿®å¤UserWarning

            # å¯é æ€§é…ç½® (ä»ç¯å¢ƒå˜é‡)
            max_retries=kwargs.get("max_retries", self.MAX_RETRIES),
            request_timeout=kwargs.get("request_timeout", self.MAX_TIMEOUT),

            # å®‰å…¨é…ç½® (ä»ç¯å¢ƒå˜é‡)
            stop=kwargs.get("stop", self.SAFETY_STOP_SEQUENCES),
        )
        
        # åŒ…è£…ä¸ºè¿½è¸ªç‰ˆæœ¬
        return TrackedChatOpenAI(original_llm)
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ (ä»ç¯å¢ƒå˜é‡é…ç½®)"""
        api_key = os.getenv("OPENROUTER_API_KEY", "")
        masked_key = api_key[:10] + "..." if api_key else "æœªè®¾ç½®"

        return {
            "provider": self.PROVIDER_NAME,
            "model": self.MODEL_NAME,
            "api_key": masked_key,
            "base_url": self.BASE_URL,
            "input_context": f"{self.MAX_INPUT_CONTEXT:,} tokens",
            "output_tokens": f"{self.MAX_OUTPUT_TOKENS:,} tokens",
            "timeout": f"{self.MAX_TIMEOUT}s",
            "temperature": self.TEMPERATURE,
            "top_p": self.TOP_P,
            "max_retries": self.MAX_RETRIES,
            "features": ["å·¥å…·è°ƒç”¨", "å¤§ä¸Šä¸‹æ–‡", "æ¨ç†æ¨¡å¼", "æ•°å­¦/ç¼–ç¨‹", "K8sè¿ç»´"],
            "configuration": "ç¯å¢ƒå˜é‡é…ç½® - éµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®º"
        }


# å…¨å±€é…ç½®å®ä¾‹ - ä»ç¯å¢ƒå˜é‡åˆå§‹åŒ–
gemini_config = GeminiMaxConfig()


def create_llm(**kwargs) -> ChatOpenAI:
    """
    ğŸ¯ ä¸»è¦å…¥å£ç‚¹ï¼šåˆ›å»ºGemini 2.5 Flash LLMå®ä¾‹

    ä»ç¯å¢ƒå˜é‡è¯»å–æ‰€æœ‰é…ç½®ï¼Œéµå¾ªåäºŒè¦ç´ åº”ç”¨æ–¹æ³•è®ºã€‚
    æ”¯æŒä¸åŒéƒ¨ç½²ç¯å¢ƒçš„çµæ´»é…ç½®ã€‚

    é…ç½®æ¥æºï¼š
    - æ¨¡å‹åç§°ï¼šLLM_MODEL_NAME
    - è¾“å…¥ä¸Šä¸‹æ–‡ï¼šLLM_MAX_INPUT_CONTEXT
    - è¾“å‡ºèƒ½åŠ›ï¼šLLM_MAX_OUTPUT_TOKENS
    - è¶…æ—¶æ—¶é—´ï¼šLLM_REQUEST_TIMEOUT
    - æ¸©åº¦è®¾ç½®ï¼šLLM_TEMPERATURE

    Args:
        **kwargs: å¯é€‰çš„è¦†ç›–å‚æ•°

    Returns:
        é…ç½®çš„Gemini 2.5 Flash ChatOpenAIå®ä¾‹

    Raises:
        ValueError: å½“å¿…éœ€çš„ç¯å¢ƒå˜é‡æœªè®¾ç½®æ—¶

    Examples:
        >>> llm = create_llm()  # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        >>> llm = create_llm(max_tokens=16384)  # è¦†ç›–è¾“å‡ºé™åˆ¶
    """
    return gemini_config.create_llm(**kwargs)


def get_model_info() -> Dict[str, Any]:
    """
    è·å–å½“å‰æ¨¡å‹é…ç½®ä¿¡æ¯

    Returns:
        åŒ…å«æ¨¡å‹è¯¦ç»†ä¿¡æ¯çš„å­—å…¸ï¼Œæ‰€æœ‰å€¼æ¥è‡ªç¯å¢ƒå˜é‡é…ç½®
    """
    return gemini_config.get_model_info()


def print_model_status():
    """æ‰“å°å½“å‰æ¨¡å‹çŠ¶æ€"""
    info = get_model_info()
    print(f"ğŸ¤– å½“å‰LLMæ¨¡å‹: {info['model']}")
    print(f"ğŸ”— æœåŠ¡åœ°å€: {info['base_url']}")
    print(f"ğŸ”‘ APIå¯†é’¥: {info['api_key']}")
    print(f"ğŸ“ è¾“å…¥ä¸Šä¸‹æ–‡: {info['input_context']}")
    print(f"ğŸ“¤ è¾“å‡ºèƒ½åŠ›: {info['output_tokens']}")
    print(f"â±ï¸  è¶…æ—¶è®¾ç½®: {info['timeout']}")
    print(f"ğŸ› ï¸  åŠŸèƒ½ç‰¹æ€§: {', '.join(info['features'])}")
    print(f"âš™ï¸  é…ç½®æ¨¡å¼: {info['configuration']}")


# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ LLMè°ƒç”¨åŒ…è£…å™¨
class TrackedChatOpenAI:
    """å¸¦è¿½è¸ªåŠŸèƒ½çš„ChatOpenAIåŒ…è£…å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
        
    def __getattr__(self, name):
        """ä»£ç†æ‰€æœ‰å±æ€§åˆ°åŸå§‹LLM"""
        return getattr(self.llm, name)
        
    async def ainvoke(self, input, **kwargs):
        """å¼‚æ­¥è°ƒç”¨æ—¶å¢åŠ æ—¥å¿—è¿½è¸ª"""
        from .output_utils import chain_start, chain_end, llm_request, llm_response, data_flow
        
        # æå–promptæ–‡æœ¬
        if hasattr(input, 'content'):
            prompt_text = input.content
        elif isinstance(input, str):
            prompt_text = input
        elif hasattr(input, 'messages') and input.messages:
            prompt_text = str(input.messages[-1].content) if input.messages else "æœªçŸ¥æ¶ˆæ¯æ ¼å¼"
        else:
            prompt_text = str(input)
            
        llm_call_id = chain_start("LLM", "ç”Ÿæˆå“åº”", 
                                f"æ¨¡å‹: {self.llm.model_name}, prompté•¿åº¦: {len(prompt_text)} chars")
        
        # è®°å½•LLMè¯·æ±‚
        llm_request(self.llm.model_name, prompt_text, getattr(self.llm, 'max_tokens', None))
        
        # æ•°æ®æµï¼šAgent â†’ LLM
        data_flow("AGENT", "LLM", "Promptæ–‡æœ¬", len(prompt_text))
        
        try:
            # è°ƒç”¨åŸå§‹LLM
            result = await self.llm.ainvoke(input, **kwargs)
            
            # æå–å“åº”æ–‡æœ¬
            response_text = result.content if hasattr(result, 'content') else str(result)
            
            # è®°å½•LLMå“åº”
            llm_response(self.llm.model_name, response_text, 
                        getattr(result, 'usage', {}).get('total_tokens') if hasattr(result, 'usage') else None)
            
            # æ•°æ®æµï¼šLLM â†’ Agent
            data_flow("LLM", "AGENT", "å“åº”æ–‡æœ¬", len(response_text))
            
            chain_end(llm_call_id, f"æˆåŠŸç”Ÿæˆå“åº”ï¼Œé•¿åº¦: {len(response_text)} chars")
            
            return result
            
        except Exception as e:
            chain_end(llm_call_id, "", f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def invoke(self, input, **kwargs):
        """åŒæ­¥è°ƒç”¨æ—¶å¢åŠ æ—¥å¿—è¿½è¸ª"""
        from .output_utils import chain_start, chain_end, llm_request, llm_response, data_flow
        
        # æå–promptæ–‡æœ¬
        if hasattr(input, 'content'):
            prompt_text = input.content
        elif isinstance(input, str):
            prompt_text = input
        elif hasattr(input, 'messages') and input.messages:
            prompt_text = str(input.messages[-1].content) if input.messages else "æœªçŸ¥æ¶ˆæ¯æ ¼å¼"
        else:
            prompt_text = str(input)
            
        llm_call_id = chain_start("LLM", "ç”Ÿæˆå“åº”", 
                                f"æ¨¡å‹: {self.llm.model_name}, prompté•¿åº¦: {len(prompt_text)} chars")
        
        # è®°å½•LLMè¯·æ±‚
        llm_request(self.llm.model_name, prompt_text, getattr(self.llm, 'max_tokens', None))
        
        # æ•°æ®æµï¼šAgent â†’ LLM
        data_flow("AGENT", "LLM", "Promptæ–‡æœ¬", len(prompt_text))
        
        try:
            # è°ƒç”¨åŸå§‹LLM
            result = self.llm.invoke(input, **kwargs)
            
            # æå–å“åº”æ–‡æœ¬
            response_text = result.content if hasattr(result, 'content') else str(result)
            
            # è®°å½•LLMå“åº”
            llm_response(self.llm.model_name, response_text,
                        getattr(result, 'usage', {}).get('total_tokens') if hasattr(result, 'usage') else None)
            
            # æ•°æ®æµï¼šLLM â†’ Agent
            data_flow("LLM", "AGENT", "å“åº”æ–‡æœ¬", len(response_text))
            
            chain_end(llm_call_id, f"æˆåŠŸç”Ÿæˆå“åº”ï¼Œé•¿åº¦: {len(response_text)} chars")
            
            return result
            
        except Exception as e:
            chain_end(llm_call_id, "", f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise


