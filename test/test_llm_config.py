#!/usr/bin/env python3
"""
LLMé…ç½®æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ä¸åŒæä¾›å•†çš„é…ç½®æ˜¯å¦æ­£ç¡®
"""

import os
import sys
from dotenv import load_dotenv
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
from llm_config import (
    create_llm,
    get_model_info,
    print_model_status
)


def test_provider_detection():
    """æµ‹è¯•æ¨¡å‹é…ç½®æ£€æµ‹"""
    print("ğŸ” æµ‹è¯•æ¨¡å‹é…ç½®æ£€æµ‹...")
    info = get_model_info()
    provider = info.get('provider', 'Unknown')
    print(f"   å½“å‰æä¾›å•†: {provider}")

    if 'OpenRouter' not in provider:
        print(f"âŒ é”™è¯¯: ä¸æ”¯æŒçš„æä¾›å•† {provider}")
        return False

    print("âœ… æ¨¡å‹é…ç½®æ£€æµ‹æ­£å¸¸")
    return True


def test_provider_info():
    """æµ‹è¯•æ¨¡å‹ä¿¡æ¯è·å–"""
    print("\nğŸ“‹ æµ‹è¯•æ¨¡å‹ä¿¡æ¯...")
    try:
        info = get_model_info()
        print(f"   æä¾›å•†: {info.get('provider', 'Unknown')}")
        print(f"   æœåŠ¡åœ°å€: {info.get('base_url', 'N/A')}")
        print(f"   æ¨¡å‹: {info.get('model', 'N/A')}")
        print(f"   è¾“å…¥ä¸Šä¸‹æ–‡: {info.get('input_context', 'N/A')}")
        print(f"   è¾“å‡ºèƒ½åŠ›: {info.get('output_tokens', 'N/A')}")

        print("âœ… æ¨¡å‹ä¿¡æ¯è·å–æ­£å¸¸")
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def test_llm_creation():
    """æµ‹è¯•LLMå®ä¾‹åˆ›å»º"""
    print("\nğŸ¤– æµ‹è¯•LLMå®ä¾‹åˆ›å»º...")

    try:
        print(f"   æµ‹è¯•ç¯å¢ƒé…ç½®LLMåˆ›å»º...")
        llm = create_llm()

        # éªŒè¯åŸºæœ¬å±æ€§
        assert hasattr(llm, 'model_name'), "ç¼ºå°‘model_nameå±æ€§"
        assert hasattr(llm, 'temperature'), "ç¼ºå°‘temperatureå±æ€§"
        assert hasattr(llm, 'max_tokens'), "ç¼ºå°‘max_tokenså±æ€§"
        assert hasattr(llm, 'openai_api_base'), "ç¼ºå°‘openai_api_baseå±æ€§"

        print(f"     âœ… ç¯å¢ƒé…ç½®LLM: {llm.model_name}")
        print(f"     âœ… æ¸©åº¦è®¾ç½®: {llm.temperature}")
        print(f"     âœ… æœ€å¤§è¾“å‡º: {llm.max_tokens:,} tokens")
        print(f"     âœ… APIåœ°å€: {llm.openai_api_base}")

        # æµ‹è¯•å‚æ•°è¦†ç›–
        print(f"   æµ‹è¯•å‚æ•°è¦†ç›–...")
        llm_override = create_llm(max_tokens=16384, temperature=0.1)
        assert llm_override.max_tokens == 16384, f"å‚æ•°è¦†ç›–å¤±è´¥: {llm_override.max_tokens}"
        assert llm_override.temperature == 0.1, f"æ¸©åº¦è¦†ç›–å¤±è´¥: {llm_override.temperature}"
        print(f"     âœ… å‚æ•°è¦†ç›–: max_tokens={llm_override.max_tokens}, temperature={llm_override.temperature}")

        print("âœ… LLMåˆ›å»ºæµ‹è¯•æˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ LLMåˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_k8s_llm_creation():
    """æµ‹è¯•Kubernetesä¸“ç”¨LLMåˆ›å»º"""
    print("\nâš™ï¸  æµ‹è¯•Kubernetesä¸“ç”¨LLMåˆ›å»º...")
    
    k8s_environments = [
        ("production", "ç”Ÿäº§ç¯å¢ƒ"),
        ("development", "å¼€å‘ç¯å¢ƒ"), 
        ("analysis", "åˆ†æç¯å¢ƒ")
    ]
    
    success_count = 0
    
    for env, description in k8s_environments:
        try:
            print(f"   æµ‹è¯•K8s {description} ({env})...")
            llm = create_llm()
            
            # éªŒè¯K8sç‰¹å®šé…ç½®
            assert llm.temperature <= 0.1, f"æ¸©åº¦è¿‡é«˜: {llm.temperature}"
            assert llm.max_tokens >= 3000, f"æœ€å¤§tokenè¿‡å°‘: {llm.max_tokens}"
            assert llm.stop, "ç¼ºå°‘å®‰å…¨åœæ­¢åºåˆ—"
            
            print(f"     âœ… K8s {description}: æ¸©åº¦={llm.temperature}, Token={llm.max_tokens}")
            success_count += 1
            
        except Exception as e:
            print(f"     âŒ K8s {description}: {e}")
    
    print(f"\nğŸ“Š K8s LLMåˆ›å»ºæµ‹è¯•ç»“æœ: {success_count}/{len(k8s_environments)} æˆåŠŸ")
    return success_count == len(k8s_environments)


def test_environment_variables():
    """æµ‹è¯•ç¯å¢ƒå˜é‡é…ç½®"""
    print("\nğŸ”§ æµ‹è¯•ç¯å¢ƒå˜é‡é…ç½®...")

    missing_vars = []

    # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
    required_vars = [
        "OPENROUTER_API_KEY",
        "OPENROUTER_BASE_URL",
        "LLM_MODEL_NAME"
    ]

    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
        else:
            print(f"   âœ… {var}: å·²è®¾ç½®")

    # æ£€æŸ¥å¯é€‰çš„ç¯å¢ƒå˜é‡
    optional_vars = [
        "LLM_MAX_INPUT_CONTEXT",
        "LLM_MAX_OUTPUT_TOKENS",
        "LLM_REQUEST_TIMEOUT",
        "LLM_TEMPERATURE",
        "LLM_TOP_P",
        "LLM_MAX_RETRIES",
        "LLM_SEED"
    ]

    for var in optional_vars:
        value = os.getenv(var)
        if value:
            print(f"   âš™ï¸  {var}: {value}")
        else:
            print(f"   ğŸ“ {var}: ä½¿ç”¨é»˜è®¤å€¼")

    if missing_vars:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("   è¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
        return False

    print("âœ… ç¯å¢ƒå˜é‡é…ç½®æ­£å¸¸")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª LLMé…ç½®æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    print_model_status()
    print("=" * 60)
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        test_provider_detection,
        test_environment_variables,
        test_provider_info,
        test_llm_creation,
        test_k8s_llm_creation
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_func.__name__} å¤±è´¥: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return 1


if __name__ == "__main__":
    sys.exit(main())
