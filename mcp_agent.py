#!/usr/bin/env python3
"""
MCP Agent å·¥ä½œç‰ˆæœ¬
ä½¿ç”¨ä¿å®ˆæ¨¡å¼é…ç½®ï¼Œç¡®ä¿ç¨³å®šè¿è¡Œ
"""

import sys
import os
import asyncio
from dotenv import load_dotenv

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from mcp_use import MCPAgent, MCPClient
from src.llm_config import create_llm, print_provider_status, get_current_provider


async def main():
    """ä¸»å‡½æ•° - ç¨³å®šçš„MCP Agentç‰ˆæœ¬"""
    # Load environment variables
    load_dotenv()

    print("ğŸš€ MCP Agent å¯åŠ¨ï¼ˆç¨³å®šç‰ˆæœ¬ï¼‰")
    print("=" * 60)
    print("ğŸ“ æ³¨æ„: ä½¿ç”¨ä¿å®ˆæ¨¡å¼é…ç½®ï¼Œç¡®ä¿ç¨³å®šè¿è¡Œ")
    print("=" * 60)
    
    # æ˜¾ç¤ºå½“å‰é…ç½®çŠ¶æ€
    print_provider_status()
    print("=" * 60)

    # æ ¹æ®é…ç½®åˆ›å»ºLLMå®ä¾‹
    print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– {get_current_provider().upper()} LLM...")
    
    # ä½¿ç”¨ä¿å®ˆé…ç½®ï¼Œç¦ç”¨å·¥å…·è°ƒç”¨ä»¥é¿å…å‚æ•°éªŒè¯é—®é¢˜
    llm = create_llm(
        model_type="production",
        temperature=0.0,
        max_tokens=4096,
        model_kwargs={
            "tool_choice": "none",  # ç¦ç”¨å·¥å…·è°ƒç”¨
            "parallel_tool_calls": False,
        }
    )
    
    print(f"âœ… LLMåˆå§‹åŒ–å®Œæˆ")
    print(f"   æ¨¡å‹: {llm.model_name}")
    print(f"   æ¸©åº¦: {llm.temperature}")
    print(f"   æœ€å¤§Token: {llm.max_tokens}")
    print(f"   æœåŠ¡åœ°å€: {llm.openai_api_base}")
    print("=" * 60)

    # å°è¯•è¿æ¥MCPæœåŠ¡å™¨
    try:
        # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®MCPæœåŠ¡å™¨
        config = {
            "mcpServers": {
                "k8s": {
                    "type": "sse",
                    "url": os.getenv("MCP_SERVER_URL", "http://localhost:31455/sse")
                }
            }
        }

        # Create MCPClient from config file
        print("ğŸ”— è¿æ¥MCPæœåŠ¡å™¨...")
        client = MCPClient.from_dict(config)

        # Create agent with the client
        print("ğŸ¤– åˆ›å»ºMCP Agent...")
        agent = MCPAgent(llm=llm, client=client, max_steps=10)

        # äº¤äº’å¼æŸ¥è¯¢å¾ªç¯
        print("âœ… MCP Agent å‡†å¤‡å°±ç»ª!")
        print("ğŸ’¡ æ‚¨å¯ä»¥è¯¢é—®å…³äºKubernetesé›†ç¾¤çš„ä»»ä½•é—®é¢˜")
        print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("=" * 60)
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\nğŸ” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                if not user_input:
                    continue
                
                print(f"\nğŸ¤– æ­£åœ¨å¤„ç†: {user_input}")
                print("-" * 40)
                
                # è¿è¡ŒæŸ¥è¯¢
                result = await agent.run(
                    user_input,
                    max_steps=5,
                )
                
                print(f"\nğŸ“‹ å›ç­”:")
                print(result)
                print("-" * 40)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
                break
            except Exception as e:
                print(f"\nâŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
                print("ğŸ’¡ è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜")
        
    except Exception as e:
        print(f"âŒ MCPè¿æ¥å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥MCPæœåŠ¡å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        print("ğŸ”§ å»ºè®®ä½¿ç”¨ç‹¬ç«‹æ¨¡å¼: uv run python standalone.py")
        
        # è¿è¡Œç®€å•çš„LLMæµ‹è¯•
        print("\nğŸ§ª è¿è¡Œç®€å•LLMæµ‹è¯•...")
        try:
            response = await llm.ainvoke("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
            print(f"ğŸ¤– LLMå›å¤: {response.content[:200]}...")
            print("âœ… LLMåŠŸèƒ½æ­£å¸¸!")
        except Exception as llm_error:
            print(f"âŒ LLMæµ‹è¯•å¤±è´¥: {llm_error}")


if __name__ == "__main__":
    asyncio.run(main())
