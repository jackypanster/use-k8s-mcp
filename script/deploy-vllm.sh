#!/bin/bash
# VLLMéƒ¨ç½²è„šæœ¬ - æ”¯æŒå·¥å…·è°ƒç”¨åŠŸèƒ½
# æ›´æ–°ç‰ˆæœ¬ï¼šæ·»åŠ äº†å·¥å…·è°ƒç”¨ç›¸å…³å‚æ•°

echo "ğŸš€ éƒ¨ç½²Qwen3-32B VLLMæœåŠ¡å™¨ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰..."

# åœæ­¢å¹¶åˆ é™¤ç°æœ‰å®¹å™¨
echo "ğŸ”„ åœæ­¢ç°æœ‰å®¹å™¨..."
docker stop coder 2>/dev/null || true
docker rm coder 2>/dev/null || true

# å¯åŠ¨æ–°å®¹å™¨
echo "ğŸ”§ å¯åŠ¨VLLMå®¹å™¨..."
docker run -d \
  --runtime=nvidia \
  --gpus=all \
  --name coder \
  -v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ \
  -p 8000:8000 \
  --cpuset-cpus 0-55 \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  --restart always \
  --ipc=host \
  vllm/vllm-openai:v0.8.5 \
  --model /model/Qwen3-32B-AWQ \
  --served-model-name coder \
  --tensor-parallel-size 4 \
  --dtype half \
  --quantization awq \
  --max-model-len 32768 \
  --max-num-batched-tokens 4096 \
  --gpu-memory-utilization 0.93 \
  --block-size 32 \
  --enable-chunked-prefill \
  --swap-space 16 \
  --tokenizer-pool-size 56 \
  --disable-custom-all-reduce \
  --enable-auto-tool-choice \
  --tool-call-parser hermes

echo "âœ… VLLMå®¹å™¨å¯åŠ¨å®Œæˆ"
echo "ğŸ”— æœåŠ¡åœ°å€: http://10.49.121.127:8000"
echo "ğŸ“‹ æ¨¡å‹åç§°: coder"
echo "ğŸ› ï¸  å·¥å…·è°ƒç”¨: å·²å¯ç”¨"

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 10

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "ğŸ” æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
if curl -s http://10.49.121.127:8000/v1/models > /dev/null; then
    echo "âœ… VLLMæœåŠ¡å¯åŠ¨æˆåŠŸ!"
    echo "ğŸ“‹ å¯ç”¨æ¨¡å‹:"
    curl -s http://10.49.121.127:8000/v1/models | jq '.data[].id' 2>/dev/null || echo "   - coder"
else
    echo "âŒ VLLMæœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—:"
    echo "   docker logs coder"
fi